{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MICpbyE9kcL5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uIEZ9kzqkmPd"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('A:/NLP/text_preprocessing/movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkOhMu6dopmF",
    "outputId": "cac74c6d-bd6b-4858-cab9-9d460c984c6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "OrIAyzi4ziAU",
    "outputId": "0febce53-2081-4889-a794-645a128b2bcc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>Imprisoned in the 1940s for the double murder ...</td>\n",
       "      <td>Drama, Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>Spanning the years 1945 to 1955, a chronicle o...</td>\n",
       "      <td>Drama, Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KPop Demon Hunters</td>\n",
       "      <td>When K-pop superstars Rumi, Mira and Zoey aren...</td>\n",
       "      <td>Animation, Fantasy, Action, Comedy, Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>In the continuing saga of the Corleone crime f...</td>\n",
       "      <td>Drama, Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>The true story of how businessman Oskar Schind...</td>\n",
       "      <td>Drama, History, War</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title  \\\n",
       "0  The Shawshank Redemption   \n",
       "1             The Godfather   \n",
       "2        KPop Demon Hunters   \n",
       "3     The Godfather Part II   \n",
       "4          Schindler's List   \n",
       "\n",
       "                                            overview  \\\n",
       "0  Imprisoned in the 1940s for the double murder ...   \n",
       "1  Spanning the years 1945 to 1955, a chronicle o...   \n",
       "2  When K-pop superstars Rumi, Mira and Zoey aren...   \n",
       "3  In the continuing saga of the Corleone crime f...   \n",
       "4  The true story of how businessman Oskar Schind...   \n",
       "\n",
       "                                       genre  \n",
       "0                               Drama, Crime  \n",
       "1                               Drama, Crime  \n",
       "2  Animation, Fantasy, Action, Comedy, Music  \n",
       "3                               Drama, Crime  \n",
       "4                        Drama, History, War  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   title     10000 non-null  object\n",
      " 1   overview  9998 non-null   object\n",
      " 2   genre     10000 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 234.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGL0v5lv9P-e"
   },
   "source": [
    "# **`Text Preprocessing`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqYZswDH9dAq"
   },
   "source": [
    "*`Lower Casing`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "g4HFsfmq7sXz",
    "outputId": "4e965664-f539-4e4a-b2e9-41ce81067808"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    imprisoned in the 1940s for the double murder ...\n",
       "1    spanning the years 1945 to 1955, a chronicle o...\n",
       "2    when k-pop superstars rumi, mira and zoey aren...\n",
       "3    in the continuing saga of the corleone crime f...\n",
       "4    the true story of how businessman oskar schind...\n",
       "Name: overview, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"overview\"] = df['overview'].str.lower()\n",
    "df['overview'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTgoR25V-NXi"
   },
   "source": [
    "`HTML tags removal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "df['overview'].str.contains('<.*?>').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hiSgrSIo9yKX",
    "outputId": "d047d7f5-6f63-4658-e782-503f5e49f0b3"
   },
   "source": [
    "Contains no HTML tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`URLs Removal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overview'].str.contains(r'https?://\\S+|www\\.\\S+').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Punctuation remval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['overview'] = df['overview'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>imprisoned in the 1940s for the double murder ...</td>\n",
       "      <td>Drama, Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>spanning the years 1945 to 1955 a chronicle of...</td>\n",
       "      <td>Drama, Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KPop Demon Hunters</td>\n",
       "      <td>when kpop superstars rumi mira and zoey arent ...</td>\n",
       "      <td>Animation, Fantasy, Action, Comedy, Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>in the continuing saga of the corleone crime f...</td>\n",
       "      <td>Drama, Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>the true story of how businessman oskar schind...</td>\n",
       "      <td>Drama, History, War</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title  \\\n",
       "0  The Shawshank Redemption   \n",
       "1             The Godfather   \n",
       "2        KPop Demon Hunters   \n",
       "3     The Godfather Part II   \n",
       "4          Schindler's List   \n",
       "\n",
       "                                            overview  \\\n",
       "0  imprisoned in the 1940s for the double murder ...   \n",
       "1  spanning the years 1945 to 1955 a chronicle of...   \n",
       "2  when kpop superstars rumi mira and zoey arent ...   \n",
       "3  in the continuing saga of the corleone crime f...   \n",
       "4  the true story of how businessman oskar schind...   \n",
       "\n",
       "                                       genre  \n",
       "0                               Drama, Crime  \n",
       "1                               Drama, Crime  \n",
       "2  Animation, Fantasy, Action, Comedy, Music  \n",
       "3                               Drama, Crime  \n",
       "4                        Drama, History, War  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Chat slang`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_slang = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
    "    \"ILU\": \"I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laughing My A** Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A**\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA?\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A** Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F***\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laugher\",\n",
    "    \"TFW\": \"That feeling when\",\n",
    "    \"MFW\": \"My face when\",\n",
    "    \"MRW\": \"My reaction when\",\n",
    "    \"IFYP\": \"I feel your pain\",\n",
    "    \"TNTL\": \"Trying not to laugh\",\n",
    "    \"JK\": \"Just kidding\",\n",
    "    \"IDC\": \"I donâ€™t care\",\n",
    "    \"ILY\": \"I love you\",\n",
    "    \"IMU\": \"I miss you\",\n",
    "    \"ADIH\": \"Another day in hell\",\n",
    "    \"ZZZ\": \"Sleeping, bored, tired\",\n",
    "    \"WYWH\": \"Wish you were here\",\n",
    "    \"TIME\": \"Tears in my eyes\",\n",
    "    \"BAE\": \"Before anyone else\",\n",
    "    \"FIMH\": \"Forever in my heart\",\n",
    "    \"BSAAW\": \"Big smile and a wink\",\n",
    "    \"BWL\": \"Bursting with laughter\",\n",
    "    \"BFF\": \"Best friends forever\",\n",
    "    \"CSL\": \"Canâ€™t stop laughing\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slang_converter(text):\n",
    "    converted_words = []\n",
    "    for word in text.split():\n",
    "        if word.upper() in chat_slang:\n",
    "            converted_words.append(chat_slang[word.upper()])\n",
    "        else:\n",
    "            converted_words.append(word)\n",
    "\n",
    "    return ' '.join(converted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"overview\"] = df[\"overview\"].apply(slang_converter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Spelling Correction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>imprisoned in the 1940s for the double murder ...</td>\n",
       "      <td>Drama, Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>spanning the years 1945 to 1955 a chronicle of...</td>\n",
       "      <td>Drama, Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KPop Demon Hunters</td>\n",
       "      <td>when kpop superstars rumi mira and zoey arent ...</td>\n",
       "      <td>Animation, Fantasy, Action, Comedy, Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>in the continuing saga of the corleone crime f...</td>\n",
       "      <td>Drama, Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>the true story of how businessman oskar schind...</td>\n",
       "      <td>Drama, History, War</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title  \\\n",
       "0  The Shawshank Redemption   \n",
       "1             The Godfather   \n",
       "2        KPop Demon Hunters   \n",
       "3     The Godfather Part II   \n",
       "4          Schindler's List   \n",
       "\n",
       "                                            overview  \\\n",
       "0  imprisoned in the 1940s for the double murder ...   \n",
       "1  spanning the years 1945 to 1955 a chronicle of...   \n",
       "2  when kpop superstars rumi mira and zoey arent ...   \n",
       "3  in the continuing saga of the corleone crime f...   \n",
       "4  the true story of how businessman oskar schind...   \n",
       "\n",
       "                                       genre  \n",
       "0                               Drama, Crime  \n",
       "1                               Drama, Crime  \n",
       "2  Animation, Fantasy, Action, Comedy, Music  \n",
       "3                               Drama, Crime  \n",
       "4                        Drama, History, War  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrected = []\n",
    "# for text in df['overview']:\n",
    "#     corrected.append(TextBlob(text).correct().string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Stop Words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AliRaza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))  # Use a set for O(1) lookup\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['overview'] = df['overview'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Taylor Swift: Reputation Stadium Tour</td>\n",
       "      <td>taylor swift takes stage dallas reputation sta...</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6448</th>\n",
       "      <td>Alice</td>\n",
       "      <td>alice tate mother two marriage 16 years finds ...</td>\n",
       "      <td>Fantasy, Drama, Romance, Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7695</th>\n",
       "      <td>Goosebumps 2: Haunted Halloween</td>\n",
       "      <td>careful wish school junk business best friends...</td>\n",
       "      <td>Comedy, Fantasy, Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3031</th>\n",
       "      <td>Gangs of Wasseypur - Part 1</td>\n",
       "      <td>1970s india sardar khan vows take revenge man ...</td>\n",
       "      <td>Action, Thriller, Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5740</th>\n",
       "      <td>Save the Last Dance</td>\n",
       "      <td>death mother sara moves south side chicago liv...</td>\n",
       "      <td>Drama, Family, Romance, Music</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title  \\\n",
       "89    Taylor Swift: Reputation Stadium Tour   \n",
       "6448                                  Alice   \n",
       "7695        Goosebumps 2: Haunted Halloween   \n",
       "3031            Gangs of Wasseypur - Part 1   \n",
       "5740                    Save the Last Dance   \n",
       "\n",
       "                                               overview  \\\n",
       "89    taylor swift takes stage dallas reputation sta...   \n",
       "6448  alice tate mother two marriage 16 years finds ...   \n",
       "7695  careful wish school junk business best friends...   \n",
       "3031  1970s india sardar khan vows take revenge man ...   \n",
       "5740  death mother sara moves south side chicago liv...   \n",
       "\n",
       "                                genre  \n",
       "89                              Music  \n",
       "6448  Fantasy, Drama, Romance, Comedy  \n",
       "7695          Comedy, Fantasy, Horror  \n",
       "3031          Action, Thriller, Crime  \n",
       "5740    Drama, Family, Romance, Music  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # Emoticons ðŸ˜€-ðŸ™\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs ðŸŒ€-ðŸ—¿\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols ðŸš€-ðŸš§\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # Flags ðŸ‡¦-ðŸ‡¿\n",
    "        u\"\\U00002700-\\U000027BF\"  # Dingbats âœ€-âž¿\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental symbols ðŸ¤-ðŸ§¿\n",
    "        u\"\\U00002600-\\U000026FF\"  # Misc symbols â˜€-â›¿\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and pictographs extended ðŸ©°-ðŸ›¸\n",
    "        u\"\\U000025A0-\\U000025FF\"  # Geometric Shapes â– -â—¿\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['overview'] = df['overview'].apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['armies',\n",
       " 'mass',\n",
       " 'final',\n",
       " 'battle',\n",
       " 'decide',\n",
       " 'fate',\n",
       " 'worldand',\n",
       " 'powerful',\n",
       " 'ancient',\n",
       " 'forces',\n",
       " 'light',\n",
       " 'dark',\n",
       " 'compete',\n",
       " 'determine',\n",
       " 'outcomeone',\n",
       " 'member',\n",
       " 'fellowship',\n",
       " 'ring',\n",
       " 'revealed',\n",
       " 'noble',\n",
       " 'heir',\n",
       " 'throne',\n",
       " 'kings',\n",
       " 'men',\n",
       " 'yet',\n",
       " 'sole',\n",
       " 'hope',\n",
       " 'triumph',\n",
       " 'evil',\n",
       " 'lies',\n",
       " 'brave',\n",
       " 'hobbit',\n",
       " 'frodo',\n",
       " 'accompanied',\n",
       " 'loyal',\n",
       " 'friend',\n",
       " 'sam',\n",
       " 'hideous',\n",
       " 'wretched',\n",
       " 'gollum',\n",
       " 'ventures',\n",
       " 'deep',\n",
       " 'dark',\n",
       " 'heart',\n",
       " 'mordor',\n",
       " 'seemingly',\n",
       " 'impossible',\n",
       " 'quest',\n",
       " 'destroy',\n",
       " 'ring',\n",
       " 'power\\u200b']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overview'].str.split()[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, prefer_gpu, require_cpu, require_gpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattributeruler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttributeRuler\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdep_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DependencyParser\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01medit_tree_lemmatizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EditTreeLemmatizer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\attributeruler.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Matcher\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\language.py:46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_EXCEPTIONS, URL_MATCH\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlookups\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_lookups\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipe_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze_pipes, print_pipe_analysis, validate_attrs\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschemas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     48\u001b[0m     ConfigSchema,\n\u001b[0;32m     49\u001b[0m     ConfigSchemaInit,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m     validate_init_settings,\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\pipe_analysis.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m msg\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc, Span, Token\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dot_to_dict\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# This lets us add type hints for mypy etc. without causing circular imports\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\tokens\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_serialize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocBin\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmorphanalysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MorphAnalysis\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\tokens\\_serialize.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleFrozenList, ensure_path\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocab\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dict_proxies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpanGroups\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DOCBIN_ALL_ATTRS \u001b[38;5;28;01mas\u001b[39;00m ALL_ATTRS\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\vocab.pyx:1\u001b[0m, in \u001b[0;36minit spacy.vocab\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:49\u001b[0m, in \u001b[0;36minit spacy.tokens.doc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\schemas.py:195\u001b[0m\n\u001b[0;32m    191\u001b[0m         obj \u001b[38;5;241m=\u001b[39m converted\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate(TokenPatternSchema, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpattern\u001b[39m\u001b[38;5;124m\"\u001b[39m: obj})\n\u001b[1;32m--> 195\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTokenPatternString\u001b[39;00m(BaseModel):\n\u001b[0;32m    196\u001b[0m     REGEX: Optional[Union[StrictStr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenPatternString\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m Field(\u001b[38;5;28;01mNone\u001b[39;00m, alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    197\u001b[0m     IN: Optional[List[StrictStr]] \u001b[38;5;241m=\u001b[39m Field(\u001b[38;5;28;01mNone\u001b[39;00m, alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\main.py:286\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[1;34m(mcs, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__signature__ \u001b[38;5;241m=\u001b[39m ClassAttribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__signature__\u001b[39m\u001b[38;5;124m'\u001b[39m, generate_model_signature(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m, fields, config))\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolve_forward_refs:\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__try_update_forward_refs__()\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# preserve `__set_name__` protocol defined in https://peps.python.org/pep-0487\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# for attributes not in `new_namespace` (e.g. private attributes)\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, obj \u001b[38;5;129;01min\u001b[39;00m namespace\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\main.py:808\u001b[0m, in \u001b[0;36mBaseModel.__try_update_forward_refs__\u001b[1;34m(cls, **localns)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__try_update_forward_refs__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlocalns: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    804\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;124;03m    Same as update_forward_refs but will not raise exception\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;124;03m    when forward references are not defined.\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 808\u001b[0m     update_model_forward_refs(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__fields__\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__config__\u001b[38;5;241m.\u001b[39mjson_encoders, localns, (\u001b[38;5;167;01mNameError\u001b[39;00m,))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\typing.py:554\u001b[0m, in \u001b[0;36mupdate_model_forward_refs\u001b[1;34m(model, fields, json_encoders, localns, exc_to_suppress)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields:\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 554\u001b[0m         update_field_forward_refs(f, globalns\u001b[38;5;241m=\u001b[39mglobalns, localns\u001b[38;5;241m=\u001b[39mlocalns)\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exc_to_suppress:\n\u001b[0;32m    556\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\typing.py:529\u001b[0m, in \u001b[0;36mupdate_field_forward_refs\u001b[1;34m(field, globalns, localns)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39msub_fields:\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sub_f \u001b[38;5;129;01min\u001b[39;00m field\u001b[38;5;241m.\u001b[39msub_fields:\n\u001b[1;32m--> 529\u001b[0m         update_field_forward_refs(sub_f, globalns\u001b[38;5;241m=\u001b[39mglobalns, localns\u001b[38;5;241m=\u001b[39mlocalns)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mdiscriminator_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     field\u001b[38;5;241m.\u001b[39mprepare_discriminated_union_sub_fields()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\typing.py:520\u001b[0m, in \u001b[0;36mupdate_field_forward_refs\u001b[1;34m(field, globalns, localns)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mtype_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m==\u001b[39m ForwardRef:\n\u001b[0;32m    519\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     field\u001b[38;5;241m.\u001b[39mtype_ \u001b[38;5;241m=\u001b[39m evaluate_forwardref(field\u001b[38;5;241m.\u001b[39mtype_, globalns, localns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mouter_type_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m==\u001b[39m ForwardRef:\n\u001b[0;32m    522\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pydantic\\v1\\typing.py:66\u001b[0m, in \u001b[0;36mevaluate_forwardref\u001b[1;34m(type_, globalns, localns)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_forwardref\u001b[39m(type_: ForwardRef, globalns: Any, localns: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Even though it is the right signature for python 3.9, mypy complains with\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# `error: Too many arguments for \"_evaluate\" of \"ForwardRef\"` hence the cast...\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Any, type_)\u001b[38;5;241m.\u001b[39m_evaluate(globalns, localns, \u001b[38;5;28mset\u001b[39m())\n",
      "\u001b[1;31mTypeError\u001b[0m: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nlp(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverview\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "nlp(df[\"overview\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
